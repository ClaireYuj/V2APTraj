GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)GATraj(
  (Temperal_Encoder): Temperal_Encoder(
    (conv1d): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=64, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp1): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (mlp): MLP(
      (linear): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm): LayerNorm()
    )
    (lstm): LSTM(64, 64, batch_first=True)
  )
  (Laplacian_Decoder): Laplacian_Decoder(
    (_decoder): GRUDecoder(
      (lstm): LSTM(64, 64)
      (loc): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (scale): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=2, bias=True)
      )
      (pi): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (multihead_proj_global): Sequential(
        (0): Linear(in_features=64, out_features=1280, bias=True)
        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (Global_interaction): ModuleList(
    (0): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
    (1): Global_interaction(
      (ngate): MLP_gate(
        (linear): Linear(in_features=192, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (relativeLayer): MLP(
        (linear): Linear(in_features=2, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
      (WAr): MLP(
        (linear): Linear(in_features=192, out_features=1, bias=True)
        (layer_norm): LayerNorm()
      )
      (weight): MLP(
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (layer_norm): LayerNorm()
      )
    )
  )
  (reg_loss): LaplaceNLLLoss()
  (cls_loss): SoftTargetCrossEntropyLoss()
)